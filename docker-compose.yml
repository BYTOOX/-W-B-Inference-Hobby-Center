services:
  # ==========================================================================
  # Ollama - LLM Inference with AMD ROCm GPU (GFX1151)
  # ==========================================================================
  ollama:
    image: ollama/ollama:rocm
    container_name: ollama
    restart: unless-stopped
    network_mode: host
    volumes:
      - /srv/docker/ollama:/root/.ollama
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    environment:
      - OLLAMA_HOST=0.0.0.0
      # GFX1151 = Radeon 8060S (Strix Point)
      - HSA_OVERRIDE_GFX_VERSION=11.0.1
      - ROCR_VISIBLE_DEVICES=0
      - HIP_VISIBLE_DEVICES=0
      # Force larger memory allocation for unified memory
      - GPU_MAX_ALLOC_PERCENT=100
      - GPU_SINGLE_ALLOC_PERCENT=100
      - GPU_MAX_HEAP_SIZE=96G
      - HSA_ENABLE_SDMA=0
      # Allow using system RAM as GPU memory
      - OLLAMA_GPU_OVERHEAD=0
    group_add:
      - video

  # ==========================================================================
  # RyzenAI-LocalLab - FastAPI Backend + Frontend
  # ==========================================================================
  locallab:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: locallab
    restart: unless-stopped
    network_mode: host
    volumes:
      - /srv/models:/srv/models
      - /srv/docker/locallab/data:/app/data
    environment:
      - MODELS_PATH=/srv/models
      - DATA_PATH=/app/data
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - SECRET_KEY=${SECRET_KEY:-changeme-in-production}
      - FIRST_ADMIN_USERNAME=${ADMIN_USER:-admin}
      - FIRST_ADMIN_PASSWORD=${ADMIN_PASS:-changeme}
      # Use localhost since both are on host network
      - OLLAMA_HOST=http://127.0.0.1:11434
      - OLLAMA_ONLY=true
      - LOG_LEVEL=INFO
    depends_on:
      - ollama
