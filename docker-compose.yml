services:
  # ==========================================================================
  # RyzenAI-LocalLab - FastAPI Backend + Frontend
  # Running alongside native Windows Ollama
  # ==========================================================================
  locallab:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: locallab
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      # Map local folders for persistence
      - ./models:/srv/models
      - ./data:/app/data
    environment:
      - MODELS_PATH=/srv/models
      - DATA_PATH=/app/data
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - SECRET_KEY=${SECRET_KEY:-changeme-in-production}
      - FIRST_ADMIN_USERNAME=${ADMIN_USER:-admin}
      - FIRST_ADMIN_PASSWORD=${ADMIN_PASS:-changeme}
      # Point to native Windows Ollama instance
      - OLLAMA_HOST=http://host.docker.internal:11434
      - OLLAMA_ONLY=true
      - LOG_LEVEL=INFO
