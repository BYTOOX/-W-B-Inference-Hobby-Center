services:
  # ==========================================================================
  # Ollama - LLM Inference with AMD ROCm GPU (GFX1151)
  # ==========================================================================
  ollama:
    image: ollama/ollama:rocm
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - /srv/docker/ollama:/root/.ollama
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    environment:
      - OLLAMA_HOST=0.0.0.0
      # GFX1151 = Radeon 8060S (Strix Point) - use 11.0.1
      - HSA_OVERRIDE_GFX_VERSION=11.0.1
      # Force GPU usage
      - ROCR_VISIBLE_DEVICES=0
      - HIP_VISIBLE_DEVICES=0
    group_add:
      - video

  # ==========================================================================
  # RyzenAI-LocalLab - FastAPI Backend + Frontend
  # ==========================================================================
  locallab:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: locallab
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - /srv/models:/srv/models
      - /srv/docker/locallab/data:/app/data
    environment:
      - MODELS_PATH=/srv/models
      - DATA_PATH=/app/data
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - SECRET_KEY=${SECRET_KEY:-changeme-in-production}
      - FIRST_ADMIN_USERNAME=${ADMIN_USER:-admin}
      - FIRST_ADMIN_PASSWORD=${ADMIN_PASS:-changeme}
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_ONLY=true
      - LOG_LEVEL=INFO
    depends_on:
      - ollama

networks:
  default:
    name: locallab-network
